% -----------------------------------------------------------------
% 				 Linear Least Squares Estimation
% -----------------------------------------------------------------

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Linear Least Squares Estimation]
Preliminaries: I.I.D and gaussian noise

Overall Model
\begin{equation*}
y(k)={ \phi (k) }^{ T }\theta +\epsilon (k)
\end{equation*}

Least Squares cost function as sum
\begin{equation*}
\sum _{ k=1 }^{ N }{{ (y(k)-{ \phi (k) }^{ T }\theta )}^{2  } } 
\end{equation*}

Least Squares cost function
\begin{equation*}
f(\theta )={ \parallel {y  }_{N  }-{ \Phi }_{ N }\theta\parallel }_{ 2 }^{2  }
\end{equation*}

Unique minimizers
\begin{equation*}
\hat{\theta}_{LS} =arg \, \underset{ \theta \in \mathbb{R} }{ min } \, f(\theta)
\end{equation*}

\begin{equation*}
{ \theta  }^{ * }=\underbrace { { ({ \Phi  }^{ T }\Phi ) }^{ -1 }{ \Phi  }^{ T } }_{ { \Phi  }^{ + } } y
\end{equation*}
Pseudo Inverse: \qquad $\Phi ^{ + }={({ \Phi  }^{ T }\Phi ) }^{ -1 }{ \Phi  }^{ T }$\\
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Weighted Least Squares (unitless)]
For I.I.D noise: Unweight Least Squares is optimal: W=I
\begin{equation*}
\sum _{ k=1 }^{ N }\frac {{{ (y(k)-{ \phi (k) }^{ T }\theta )}^{2  } }}{\sigma_{\epsilon}^{2}(k)}
\end{equation*}
\begin{equation*}
{ f }_{ WLS }(\theta )={ \parallel { y }_{ N }-{ \Phi  }_{ N }\theta \parallel  }_{ W }^{ 2 }={ ({ Y }_{ N }-\Phi \cdot \theta ) }^{ T }\cdot W\cdot  ({ Y }_{ N }-\Phi \cdot \theta )
\end{equation*}

Solution for WLS:
\begin{equation*}
\hat{\theta}_{WLS} = \tilde{\Phi}^+\tilde{y} \\
\end{equation*}
with $\tilde{\Phi} = W^{\frac{1}{2}} \Phi$ and $\tilde{y} = W^{\frac{1}{2}} y$.


\begin{equation*}
{ \hat{\theta} }_{ WLS }=arg\, \underset{ \theta \in \mathbb{R} }{ min }\,{f  }_{WLS  }(\theta)={ ({\Phi}^{T}W\Phi) }^{ -1 }{\Phi}^{T} Wy
\end{equation*}
\end{tcolorbox}
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Ill-Posed Least Squares]
	
TODO: make understandable
	
Singular Value Decomposition
\begin{equation*}
A=US{ V }^{ T }\quad mit\quad U\in\mathbb{{R}}^{mxm}, \quad V\in\mathbb{{R}}^{nxn} und \quad S\in\mathbb{{R}}^{mxn}
\end{equation*}

Moore Penrose Pseudo Inverse
\begin{equation*}
{ \Phi  }^{ + }=V{ S }^{+}{U}^{T}
\end{equation*}

Regularization for Least Squares\\
\( { lim }_{ \alpha \rightarrow 0 }{ ({ \Phi  }^{ T }\Phi +\alpha { I }) }^{ -1 }{ \Phi  }^{ T }={ \Phi  }^{ + }\, with\,{ \Phi  }^{ + }\, MPPI \)
\begin{equation*}
{ \theta  }^{ * }(\alpha )=arg\, \underset { \theta \in { R } }{ min } \, \frac { 1 }{ 2 } {\parallel y-\Phi\theta \parallel}_{2}^{2}+\frac { \alpha }{ 2 } {\parallel \theta\parallel}_{2}^{2}
\end{equation*}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Statistical Analysis of WLS]
Expectation of Least Squares Estimator
\begin{equation*}
{ E }\{ { \hat { \theta  } _{ WLS } }\} { ={ E }\{ ({ \Phi  }_{ N }^{ T }W{ \Phi  }_{ N }) }^{ -1 }{ \Phi  }_{ N }^{ T }W{ y }_{ N }\} ={\theta}_{0}
\end{equation*}
Covariance of the least squares estimator
\begin{equation*}
cov({ \hat { \theta  } _{ WLS } }){ =({ \Phi  }_{ N }^{ T }W{ \Phi  }_{ N }) }^{ -1 }{ =({ \Phi  }_{ N }^{ T }{\Sigma}_{\in N }^{ -1 }{ \Phi  }_{ N }) }^{ -1 }
\end{equation*}

\begin{equation*}
cov({ \hat { \theta  } _{ WLS } }){ \succeq ({ \Phi  }_{ N }^{ T }W{ \Phi  }_{ N }) }^{ -1 }
\end{equation*}
\end{tcolorbox}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Example LLS]
\begin{equation*}
\varepsilon (1) \sim  { \cal{N} }(0|\sigma ^{ 2 }_{ 1 }) \quad \varepsilon (2) \sim  { \cal{N} }(0|\sigma ^{ 2 }_{ 2 })
\end{equation*}

\begin{equation*}
N=2 \quad { \Sigma  }_{ { \varepsilon  }_{ N } }=\quad \begin{bmatrix} \sigma _{ 1 }^{ 2 } & 0 \\ 0 & \sigma _{ 2 }^{ 2 } \end{bmatrix}
\end{equation*}

\begin{equation*}
W^{ OPT } = \Sigma _{ ^{ \varepsilon_{ N } } }^{ -1 }\quad \begin{bmatrix} \frac { 1 }{ \sigma _{ 1 }^{ 2 } }  & 0 \\ 0 & \frac { 1 }{ \sigma _{ 2 }^{ 2 } }  \end{bmatrix}
\end{equation*}

\begin{equation*}
cov({ \hat { \theta  } _{ WLS } }){ =({ Y }_{ N }-{ \Phi  }_{ N }\theta ) }^{ T }\cdot W\cdot ({ Y }_{ N }-{ \Phi  }_{ N }\theta )=
\end{equation*}

\begin{equation*}
\sum _{ k=1 }^{ 2 }{ (y(k)-{ \phi (k) }^{ T }\theta )\cdot \frac { 1 }{ { \sigma  }_{ k }^{ 2 } } \cdot (y(k)-{ \phi (k) }^{ T }\theta ) } 
\end{equation*}


Measuring the goodness of Fit using \({R}^{2} \quad 0\le {R}^{2} \le1\) 
\begin{equation*}
{ R }^{ 2 }=1-\frac { { \parallel { y }_{ N }-{ \Phi  }_{ N }\hat { \theta  } \parallel  }_{ 2 }^{ 2 } }{ { \parallel { y }_{ N }\parallel  }_{ 2 }^{ 2 } } =1-\frac { { \parallel { \epsilon  }_{ N }\parallel  }_{ 2 }^{ 2 } }{ { \parallel { y }_{ N }\parallel  }_{ 2 }^{ 2 } } =
\end{equation*}
\begin{equation*}
\frac { { \parallel { y }_{ N }\parallel  }_{ 2 }^{ 2 }-{ \parallel { \epsilon  }_{ N }\parallel  }_{ 2 }^{ 2 } }{ { \parallel { y }_{ N }\parallel  }_{ 2 }^{ 2 } } =\frac { { \parallel { \hat { y  }  }_{ N }\parallel  }_{ 2 }^{ 2 } }{ { \parallel { y }_{ N }\parallel  }_{ 2 }^{ 2 } } 
\end{equation*}
residual $ \epsilon_{N} \uparrow \quad \rightarrow \quad R^{2} \rightarrow 0 \,\,(= bad)$
\tcblower

Estimating the Covariance with the Single Experiment
\begin{equation*}
\hat { \sigma  } _{ \varepsilon  }^{ 2 } := \frac { 1 }{ N-d } \sum _{ k=1 }^{ N }{ (y(k)-\phi (k)^{ T }\hat { \theta  } _{ LS })^{ 2 } }  = \frac { \parallel y_{ N }-\phi _{ N }\hat { \theta  } _{ LS }{ \parallel  }_{ 2 }^{ 2 } }{ N-d } 
\end{equation*}

\begin{equation*}
\hat { \Sigma  } _{ \hat { \theta  }  } := \hat { \sigma  } ^{ 2 }_{ \varepsilon  } (\phi ^{ T }_{ N } \phi _{ N })^{ -1 } = \frac { \parallel y_{ N }\, -\phi _{ N }\hat { \theta  } _{ LS }{ \parallel  }_{ 2 }^{ 2 } }{ N-d } \cdot (\phi ^{ T }_{ N } \phi _{ N })^{ -1 }
\end{equation*}
\end{tcolorbox}
