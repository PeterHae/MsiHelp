% -----------------------------------------------------------------
% 				 Maximum Likelihood Estimation
% -----------------------------------------------------------------
\section*{Maximum Likelihood Estimation}
Maximum Likelihood Estimation (ML) \({L}_{2}\) Estimation:\\
Measurement Errors assumed to be Normally distributed
\begin{equation*}
{ P(y|\theta ) }=C\prod _{ i=1 }^{ N }{ exp(\frac { -(y_{ i }-M_{ i }(\theta ))^{ 2 } }{ 2\cdot \sigma _{ i }^{ 2 } } )} 
\end{equation*}

Positive log-Likelihood. Logarithm makes from products a sum!
\begin{equation*}
{ log \, p(y|\theta ) } = log(C) +\sum _{ i=1 }^{ N }{ { \frac { -(y_{ i }-M_{ i }(\theta ))^{ 2 } }{ 2 \cdot \sigma _{ i }^{ 2 } }  } } 
\end{equation*}

Negative log-Likelihood:
\begin{equation*}
\hat{\theta}_{ML} \, = \, arg \, \underset { \theta \in { \mathbb{R} }^{ d } }{ max } \, p(y|\theta ) = arg \, \underset { \theta \in { \mathbb{R} }^{ d } }{ min }  \sum _{ i=1 }^{ m }{ \frac { (y_{ i }-M_{ i }(\theta ))^{ 2 } }{ 2\, { \sigma _{ i } }^{ 2 } }  } 
\end{equation*}

\begin{equation*}
arg \, \underset{ \theta \in \mathbb{ R }^{ d } }{ max } \, p(y|\theta ) = arg \, \underset { \theta \in {  R }^{ d } }{ min } \quad \frac{1}{2} \parallel S^{ -1 }\cdot (y-M(\theta )){ \parallel  }_{ 2 }^{ 2 }
\end{equation*}

\({L}_{1}\) Estimation:\\
Measurement Errors assumed to be Laplace distributed.\\ \(Median(x)=\left\lceil \frac { x + 1 }{ 2 }  \right\rceil \)\\
Robust against outliers
\begin{equation*}
\underset { \theta  }{ min } { \parallel y-M(\theta )\parallel  }_{ 1 }\,=\,\underset { \theta  }{ min } \sum _{ i=1 }^{ N }{ |{ y }_{ i }-{ M }_{ i }\theta | } \,=
\end{equation*}
\begin{equation*}
=\,median\,of\,\{{Y}_{1},...,{Y}_{N}\}
\end{equation*}



\begin{equation*}
{ P(y|\theta ) }=C\prod _{ i=1 }^{ N }{ { exp }(\frac { -|{ y }_{ i }-\theta | }{ 2\cdot { a }_{ i } } ) } 
\end{equation*} 

Bayesian Estimation and the Maximum a Posteriori Estimate\\
Assumptions: i.i.d noise and linear model
\begin{equation*}
p(\theta |{ y }_{ N })\cdot p({ y }_{ N })=p({ y }_{ N }|\theta )\cdot p(\theta)
\end{equation*}

\begin{equation*}
{ \hat{\theta} }_{ MAP } = arg \, \underset { \theta \in \mathbb{R} }{ min } \{-log(p({ y }_{ N }|\theta))-log(p(\theta))\}
\end{equation*}

MAP Example: Regularised Least Squares
\begin{equation*}
\theta =\overline { \theta  } \pm { \sigma  }_{ \theta  }\quad with \quad \overline { \theta  } = { \theta  }_{ apriori  }
\end{equation*}

\begin{equation*}
{ \hat { \theta  }  }_{ MAP }=arg\, \underset { \theta \in { R } }{ min } \frac { 1 }{ 2 } \cdot \frac { 1 }{ { \sigma  }_{ \epsilon  }^{ 2 } } \cdot { \parallel { y }_{ N }-{ \Phi  }_{ N }\cdot \theta \parallel  }_{ 2 }^{ 2 }+\frac { 1 }{ 2 } \cdot \frac { 1 }{ { \sigma  }_{ \theta  }^{ 2 } } \cdot { (\theta -  \overline{\theta}) }^{ 2 }
\end{equation*}

\subsection*{Recursive Linear Least Squares}
\begin{equation*}
\theta _{ ML }(N) \, = \, arg\, \underset { \theta \, \in \, { R } }{ min } \, \frac { 1 }{ 2 } \parallel y_N - \Phi_N\cdot\,\theta{ \parallel  }_{ 2 }^{ 2 }
\end{equation*}

\begin{equation*}
\hat { \theta  } _{ ML }\, (N+1)\, =\, arg\, \underset { \theta \, \in \, { R^{ d } } }{ min } \, (\alpha \, \cdot \, \frac { 1 }{ 2 } \, \cdot \, \parallel \theta \, - \, \hat { \theta  } _{ ML }(N){ \parallel  }_{ Q_{ N } }^{ 2 }+
\end{equation*}
\begin{equation*}
\frac { 1 }{ 2 } \, \cdot \, \parallel y(N+1) \, - \, \varphi (N+1)^{ T }\, \cdot \, \theta \, { \parallel  }_{ 2 }^{ 2 })
\end{equation*}

\begin{equation*}
Q_{ 0 }\quad given,\quad and\quad \hat { \theta  } _{ ML }(0)\quad given,
\end{equation*}
\begin{equation*}
Q_{ N+1 }\, =\, \alpha \, \cdot \, Q_{ N }+\varphi (N+1)\, \cdot \, \varphi (N+1)^{ T },
\end{equation*}
\begin{equation*}
\hat { \theta  } _{ ML }(N+1)\, =\, \hat { \theta  } _{ ML }(N)+Q_{ N+1 }^{ -1 }\, \cdot \, \varphi (N+1)\, \cdot \, [y(N+1)-
\end{equation*}
\begin{equation*}
\varphi(N+1)^T \, \cdot \,\hat{\theta}_{ML}(N)]
\end{equation*}

\subsection*{Cramer-Rao-Inequality (Fisher information Matrix M)}
\begin{equation*}
{ \Sigma  }_{ \theta  } \succeq M^{-1} = (\Phi^T_N \cdot \Sigma^{-1} \cdot \Phi)^{-1}
\end{equation*}
\begin{equation*}
L(\theta ,y_{ N })\, =\, \frac { 1 }{ 2 } \, \cdot \, (\Phi _{ N }\, \cdot \, \theta \, -\, y_N)^{ T } \, \cdot \, \Sigma^{-1} \, \cdot \,  (\Phi_N \, \cdot \, \theta \, - \, y_N) =log\, (p({ y }_{ N }|\theta))
\end{equation*}
\begin{equation*}
M\, =\, { E }\{ \triangledown ^{ 2 }_{ \theta  }\, L(\theta ,y_{ N })\} \, =\, \triangledown ^{ 2 }_{ \theta  }\, L(\theta ,y_{ N })\, =\, \Phi _{ N }^{ T }\, \cdot \, \Sigma ^{ -1 }\, \cdot \, \Phi _{ N }
\end{equation*}
Confirms that $W = \Sigma^{-1}$ is the optimal weighting Matrix for WLS.